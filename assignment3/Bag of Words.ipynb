{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.cluster.vq import vq,kmeans\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 1\n",
    "#Choose three different categories of objects from the Caltech 101 dataset, as diverse from each other as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset_metadata(dir_path):\n",
    "    dataset = {}\n",
    "    for filename in os.listdir(dir_path):\n",
    "        if os.path.isdir(dir_path + filename):\n",
    "            dataset[filename] = []\n",
    "            for img in os.listdir(os.path.join(dir_path + filename)):\n",
    "                dataset[filename].append(os.path.join(dir_path,filename,img))\n",
    "    return dataset\n",
    "\n",
    "def get_frequent_object(dataset,count=3):\n",
    "    \"\"\"\n",
    "    dataset : Dictionary object returned by the function 'read_dataset'\n",
    "    \n",
    "    Returns a list of objects with their frequency in descending order\n",
    "    \"\"\"\n",
    "    key_count = {}\n",
    "    for key in dataset.keys():\n",
    "        key_count[key] = len(dataset[key])\n",
    "    \n",
    "    \n",
    "    object_keys = []\n",
    "    for key, value in sorted(key_count.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "        object_keys.append(key)\n",
    "        count-=1\n",
    "        if count == 0:\n",
    "            break\n",
    "    return object_keys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_metadata = read_dataset_metadata('./101_ObjectCategories/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplanes', 'Motorbikes', 'BACKGROUND_Google', 'Faces_easy', 'Faces']\n"
     ]
    }
   ],
   "source": [
    "frequent_objects = get_frequent_object(dataset_metadata,5)\n",
    "print frequent_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The output of the above cell shows the 5 most frequent objects\n",
    "#In the interest of choosing three different categories as diverse from each other as possible\n",
    "#We choosse 'airplanes', 'Motorbikes' and 'Faces_easy' classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['airplanes', 'Motorbikes','Faces_easy']\n",
    "\n",
    "new_dataset_metadata = {}\n",
    "for category in categories:\n",
    "    new_dataset_metadata[category] = dataset_metadata[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset(metadata):\n",
    "    try:\n",
    "        dataset = []\n",
    "        for key in metadata.keys():\n",
    "            for img_path in metadata[key]:\n",
    "                img = cv2.imread(img_path,0)\n",
    "                dataset.append((img,key))\n",
    "        dataset = np.array(dataset)\n",
    "        np.save('dataset',dataset)\n",
    "        return 'dataset.npy'\n",
    "    except:\n",
    "        raise \"Error in creating Dataset File\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = make_dataset(new_dataset_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 2\\nExtract some local features (SIFT/SURF), cluster them using k-Means algorithm, and\\ncreate a bag-of-words representation for images. This bag-of-word representation is to be\\nused as image feature in the subsequent steps.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 2\n",
    "Extract some local features (SIFT/SURF), cluster them using k-Means algorithm, and\n",
    "create a bag-of-words representation for images. This bag-of-word representation is to be\n",
    "used as image feature in the subsequent steps.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'mask' (pos 2) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-19aa708b94a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIFT_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mkp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'mask' (pos 2) not found"
     ]
    }
   ],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "kp, des = sift.detectAndCompute(dataset[0][0],None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_list = []\n",
    "for img_data in dataset:\n",
    "    im = img_data[0]\n",
    "    kpts = sift.detect(im)\n",
    "    kpts, des = sift.compute(im, kpts)\n",
    "    des_list.append((im, des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptors = des_list[0][1]\n",
    "for img, descriptor in des_list[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619879, 128)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "voc, variance = kmeans(descriptors, k, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_features = np.zeros((len(dataset), k), \"float32\")\n",
    "for i in xrange(len(dataset)):\n",
    "    words, distance = vq(des_list[i][1],voc)\n",
    "    for w in words:\n",
    "        im_features[i][w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdSlr = StandardScaler().fit(im_features)\n",
    "im_features = stdSlr.transform(im_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.91382909, -1.30561399, -0.94648373, ..., -1.41317773,\n",
       "         0.28676912, -1.23465788],\n",
       "       [-0.71712714, -0.90231484, -0.88215715, ..., -0.83762997,\n",
       "        -0.93375099, -0.93811244],\n",
       "       [-0.71712714, -1.10396445, -1.0751369 , ..., -0.40596923,\n",
       "         1.1004492 , -0.93811244],\n",
       "       ..., \n",
       "       [ 0.83616447,  0.91253138,  0.08274174, ...,  1.75233459,\n",
       "         0.61224115,  0.24806936],\n",
       "       [ 0.91382909,  1.71912968,  1.75523317, ...,  2.18399525,\n",
       "         3.05328131,  1.92849362],\n",
       "       [ 1.53514576,  0.8117066 ,  0.98331404, ...,  0.02569152,\n",
       "         1.26318514,  2.12619042]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_descriptor_data(dataset):\n",
    "    try:\n",
    "        descriptor_list = []\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        for img_data in dataset:\n",
    "            img = img_data[0]\n",
    "            label = img_data[1]\n",
    "            #Computing descriptors for every image\n",
    "            kpts = sift.detect(im)\n",
    "            kpts, des = sift.compute(im, kpts)\n",
    "            descriptor_list.append((img, des, label))\n",
    "            \n",
    "        descriptor_list = np.array(descriptor_list)\n",
    "        np.save('descriptor_data',descriptor_list)\n",
    "        return True\n",
    "    except:\n",
    "        raise \"Error in creating Bag of Words File\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_descriptor_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_data = np.load('descriptor_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = descriptor_data[0][1]\n",
    "for img, descriptor,label in descriptor_data[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means clustering of the descriptors\n",
    "k = 20\n",
    "voc, variance = kmeans(descriptors, k, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Crearing Histogram of the Bag of Words\n",
    "im_features = np.zeros((len(dataset), k), \"float32\")\n",
    "class_labels =[]\n",
    "for i in xrange(len(dataset)):\n",
    "    words, distance = vq(descriptor_data[i][1],voc)\n",
    "    class_labels.append(descriptor_data[i][2])\n",
    "    for w in words:\n",
    "        im_features[i][w] += 1\n",
    "class_labels = np.array(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = []\n",
    "for im_feature,label in zip(im_features,class_labels):\n",
    "    feature_dataset.append((im_feature,label))\n",
    "\n",
    "feature_dataset=np.array(feature_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = np.load('bag_of_words_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 3:\\nChoose approximately half of the images from each category, as training data. Save the\\nrest as test data.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 3:\n",
    "Choose approximately half of the images from each category, as training data. Save the\n",
    "rest as test data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_map = {'airplanes':1, 'Motorbikes':2,'Faces_easy':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting bow_data according to categories\n",
    "category_bow_data = {}\n",
    "for data in feature_dataset:\n",
    "    if category_bow_data.has_key(data[1]):\n",
    "        category_bow_data[data[1]].append(data[0])\n",
    "    else:\n",
    "        category_bow_data[data[1]]=[]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into training and testing data\n",
    "x_train,y_train,x_test,y_test = [[],[],[],[]]\n",
    "category_count = {'airplanes':len(category_bow_data['airplanes']), 'Motorbikes':len(category_bow_data['Motorbikes']),\n",
    "                  'Faces_easy':len(category_bow_data['Faces_easy'])}\n",
    "for category in category_map:\n",
    "    x_train.extend(category_bow_data[category][0:category_count[category]/2+1])\n",
    "    y_train.extend([category_map[category] for _ in range(len(category_bow_data[category][0:category_count[category]/2+1]))])\n",
    "    \n",
    "    x_test.extend(category_bow_data[category][category_count[category]/2+1:])\n",
    "    y_test.extend([category_map[category] for _ in range(len(category_bow_data[category][category_count[category]/2+1:]))])\n",
    "    \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1017, 20) (1017,)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 4:\\nTrain Normal Bayesian Classifier of OpenCV to distinguish between the three object\\ncategories using the chosen training data.\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 4:\n",
    "Train Normal Bayesian Classifier of OpenCV to distinguish between the three object\n",
    "categories using the chosen training data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39387956564659427"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function vq in module scipy.cluster.vq:\n",
      "\n",
      "vq(obs, code_book, check_finite=True)\n",
      "    Assign codes from a code book to observations.\n",
      "    \n",
      "    Assigns a code from a code book to each observation. Each\n",
      "    observation vector in the 'M' by 'N' `obs` array is compared with the\n",
      "    centroids in the code book and assigned the code of the closest\n",
      "    centroid.\n",
      "    \n",
      "    The features in `obs` should have unit variance, which can be\n",
      "    achieved by passing them through the whiten function.  The code\n",
      "    book can be created with the k-means algorithm or a different\n",
      "    encoding algorithm.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    obs : ndarray\n",
      "        Each row of the 'M' x 'N' array is an observation.  The columns are\n",
      "        the \"features\" seen during each observation. The features must be\n",
      "        whitened first using the whiten function or something equivalent.\n",
      "    code_book : ndarray\n",
      "        The code book is usually generated using the k-means algorithm.\n",
      "        Each row of the array holds a different code, and the columns are\n",
      "        the features of the code.\n",
      "    \n",
      "         >>> #              f0    f1    f2   f3\n",
      "         >>> code_book = [\n",
      "         ...             [  1.,   2.,   3.,   4.],  #c0\n",
      "         ...             [  1.,   2.,   3.,   4.],  #c1\n",
      "         ...             [  1.,   2.,   3.,   4.]]  #c2\n",
      "    \n",
      "    check_finite : bool, optional\n",
      "        Whether to check that the input matrices contain only finite numbers.\n",
      "        Disabling may give a performance gain, but may result in problems\n",
      "        (crashes, non-termination) if the inputs do contain infinities or NaNs.\n",
      "        Default: True\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    code : ndarray\n",
      "        A length M array holding the code book index for each observation.\n",
      "    dist : ndarray\n",
      "        The distortion (distance) between the observation and its nearest\n",
      "        code.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from numpy import array\n",
      "    >>> from scipy.cluster.vq import vq\n",
      "    >>> code_book = array([[1.,1.,1.],\n",
      "    ...                    [2.,2.,2.]])\n",
      "    >>> features  = array([[  1.9,2.3,1.7],\n",
      "    ...                    [  1.5,2.5,2.2],\n",
      "    ...                    [  0.8,0.6,1.7]])\n",
      "    >>> vq(features,code_book)\n",
      "    (array([1, 1, 0],'i'), array([ 0.43588989,  0.73484692,  0.83066239]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
