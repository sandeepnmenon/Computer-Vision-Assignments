{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.cluster.vq import vq,kmeans\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 1\n",
    "#Choose three different categories of objects from the Caltech 101 dataset, as diverse from each other as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset_metadata(dir_path):\n",
    "    dataset = {}\n",
    "    for filename in os.listdir(dir_path):\n",
    "        if os.path.isdir(dir_path + filename):\n",
    "            dataset[filename] = []\n",
    "            for img in os.listdir(os.path.join(dir_path + filename)):\n",
    "                dataset[filename].append(os.path.join(dir_path,filename,img))\n",
    "    return dataset\n",
    "\n",
    "def get_frequent_object(dataset,count=3):\n",
    "    \"\"\"\n",
    "    dataset : Dictionary object returned by the function 'read_dataset'\n",
    "    \n",
    "    Returns a list of objects with their frequency in descending order\n",
    "    \"\"\"\n",
    "    key_count = {}\n",
    "    for key in dataset.keys():\n",
    "        key_count[key] = len(dataset[key])\n",
    "    \n",
    "    \n",
    "    object_keys = []\n",
    "    for key, value in sorted(key_count.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "        object_keys.append(key)\n",
    "        count-=1\n",
    "        if count == 0:\n",
    "            break\n",
    "    return object_keys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata = read_dataset_metadata('./101_ObjectCategories/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplanes', 'Motorbikes', 'BACKGROUND_Google', 'Faces_easy', 'Faces']\n"
     ]
    }
   ],
   "source": [
    "frequent_objects = get_frequent_object(dataset_metadata,5)\n",
    "print frequent_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The output of the above cell shows the 5 most frequent objects\n",
    "#In the interest of choosing three different categories as diverse from each other as possible\n",
    "#We choosse 'airplanes', 'Motorbikes' and 'Faces_easy' classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['airplanes', 'Motorbikes','Faces_easy']\n",
    "\n",
    "new_dataset_metadata = {}\n",
    "for category in categories:\n",
    "    new_dataset_metadata[category] = dataset_metadata[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset(metadata):\n",
    "    try:\n",
    "        dataset = []\n",
    "        for key in metadata.keys():\n",
    "            for img_path in metadata[key]:\n",
    "                img = cv2.imread(img_path,0)\n",
    "                dataset.append((img,key))\n",
    "        dataset = np.array(dataset)\n",
    "        np.save('dataset',dataset)\n",
    "        return 'True\n",
    "    except:\n",
    "        raise \"Error in creating Dataset File\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_dataset(new_dataset_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('dataset.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 2\\nExtract some local features (SIFT/SURF), cluster them using k-Means algorithm, and\\ncreate a bag-of-words representation for images. This bag-of-word representation is to be\\nused as image feature in the subsequent steps.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 2\n",
    "Extract some local features (SIFT/SURF), cluster them using k-Means algorithm, and\n",
    "create a bag-of-words representation for images. This bag-of-word representation is to be\n",
    "used as image feature in the subsequent steps.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_descriptor_data(dataset):\n",
    "    try:\n",
    "        descriptor_list = []\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        for img_data in dataset:\n",
    "            img = img_data[0]\n",
    "            label = img_data[1]\n",
    "            #Computing descriptors for every image\n",
    "            kpts = sift.detect(img)\n",
    "            kpts, des = sift.compute(img, kpts)\n",
    "            descriptor_list.append((img, des, label))\n",
    "            \n",
    "        descriptor_list = np.array(descriptor_list)\n",
    "        np.save('descriptor_data',descriptor_list)\n",
    "        return True\n",
    "    except:\n",
    "        raise \"Error in creating Bag of Words File\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_descriptor_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptor_data = np.load('descriptor_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stacking descriptors before clustering\n",
    "descriptors = descriptor_data[0][1]\n",
    "for img, descriptor,label in descriptor_data[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K-Means clustering of the descriptors\n",
    "k = 100\n",
    "voc, variance = kmeans(descriptors, k, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Crearing Histogram of the Bag of Words\n",
    "im_features = np.zeros((len(dataset), k), \"float32\")\n",
    "class_labels =[]\n",
    "for i in xrange(len(dataset)):\n",
    "    words, distance = vq(descriptor_data[i][1],voc)\n",
    "    class_labels.append(descriptor_data[i][2])\n",
    "    for w in words:\n",
    "        im_features[i][w] += 1\n",
    "        \n",
    "stdSlr = StandardScaler(with_mean=False).fit(im_features)\n",
    "im_features = stdSlr.transform(im_features)\n",
    "class_labels = np.array(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dataset = []\n",
    "for im_feature,label in zip(im_features,class_labels):\n",
    "    feature_dataset.append((im_feature,label))\n",
    "\n",
    "feature_dataset=np.array(feature_dataset)\n",
    "np.save('bag_of_words_features',feature_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dataset = np.load('bag_of_words_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
